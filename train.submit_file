####################
#
# Job for HTCondor STAR
#
####################

#---------------------------------------------
# Name your batch so it's easy to distinguish in the q.
JobBatchName = "STAR-train"

# --------------------------------------------
# Executable and its arguments
executable    = /workspace/miniconda3/envs/star/bin/python
arguments     = -m apps.run --config configs/train.yaml --text \"A pink pig, he/she is running like a rabbit\" --description pigrabbit --t2m_model mdiffuse

# ---------------------------------------------------
# Universe (vanilla, docker)
universe         = docker
docker_image = container-registry.surrey.ac.uk/shared-containers/star:latest

# -------------------------------------------------
# Event, out and error logs
log    = c$(cluster).p$(process).log
output = c$(cluster).p$(process).out
error  = c$(cluster).p$(process).error

# -----------------------------------
# File Transfer, Input, Output
should_transfer_files = YES
transfer_input_files = $ENV(PWD)/data, $ENV(PWD)/configs, $ENV(PWD)/externals
transfer_output_files = ./

# Make certain project spaces available in container
environment = "PYTHONPATH=/workspace/STAR:/workspace/STAR/lib:/workspace/STAR/externals/R2ET:/workspace/STAR/externals/MotionDiffuse:/workspace/STAR/externals/MotionDiffuse/text2motion:/workspace/STAR/externals/mmhuman3d:$PYTHONPATH"
# environment = "PYTHONPATH=/workspace/STAR:$PYTHONPATH"

# Make certain project spaces available in container
# Uncomment this environment line if you're not running on /mnt/fast
# environment = "mount=$ENV(PWD)"
# environment = "PYTHONPATH=$ENV(PWD):$PYTHONPATH:LD_LIBRARY_PATH=$ENV(PWD)/miniconda3/envs/star/x86_64-conda-linux-gnu/sysroot/usr/lib64/libGL.so.1"

# -------------------------------------
# Requirements for the Job (Requirements are explained in further detail in example09.submit_file)
# NOTE: HasStornext is not valid on orca.
requirements = (CUDAGlobalMemoryMb > 4500)

# --------------------------------------
# Resources
request_GPUs     = 1
# this needs to be specified for the AI@Surrey cluster if requesting a GPU
+GPUMem          = 42000
request_CPUs   = 3
# tailor the number of CPUs to your needs - check epoch time or GPU power usage via 
# https://wandb.ai/site to know that you're not staving the GPU
request_memory   = 12G

#This job will complete in less than 3 hours
+JobRunTime = 3

#This job can checkpoint
+CanCheckpoint = false

# -----------------------------------
# Queue commands
queue